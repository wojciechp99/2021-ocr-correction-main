{
  "best_metric": 3.333712100982666,
  "best_model_checkpoint": "./results/checkpoint-1102",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1102,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018148820326678767,
      "grad_norm": 2.845208168029785,
      "learning_rate": 4.969751966122203e-05,
      "loss": 5.7959,
      "step": 10
    },
    {
      "epoch": 0.036297640653357534,
      "grad_norm": 7.489420413970947,
      "learning_rate": 4.939503932244404e-05,
      "loss": 4.469,
      "step": 20
    },
    {
      "epoch": 0.0544464609800363,
      "grad_norm": 0.8274708986282349,
      "learning_rate": 4.909255898366607e-05,
      "loss": 4.1011,
      "step": 30
    },
    {
      "epoch": 0.07259528130671507,
      "grad_norm": 4.220892906188965,
      "learning_rate": 4.879007864488808e-05,
      "loss": 4.1379,
      "step": 40
    },
    {
      "epoch": 0.09074410163339383,
      "grad_norm": 2.3187668323516846,
      "learning_rate": 4.8487598306110105e-05,
      "loss": 4.0936,
      "step": 50
    },
    {
      "epoch": 0.1088929219600726,
      "grad_norm": 1.9971623420715332,
      "learning_rate": 4.8185117967332124e-05,
      "loss": 4.1233,
      "step": 60
    },
    {
      "epoch": 0.12704174228675136,
      "grad_norm": 0.6250284910202026,
      "learning_rate": 4.7882637628554143e-05,
      "loss": 4.0152,
      "step": 70
    },
    {
      "epoch": 0.14519056261343014,
      "grad_norm": 0.6300209760665894,
      "learning_rate": 4.758015728977617e-05,
      "loss": 4.0675,
      "step": 80
    },
    {
      "epoch": 0.16333938294010888,
      "grad_norm": 3.348555564880371,
      "learning_rate": 4.727767695099819e-05,
      "loss": 3.9673,
      "step": 90
    },
    {
      "epoch": 0.18148820326678766,
      "grad_norm": 1.829658031463623,
      "learning_rate": 4.697519661222021e-05,
      "loss": 4.0143,
      "step": 100
    },
    {
      "epoch": 0.1996370235934664,
      "grad_norm": 0.6231280565261841,
      "learning_rate": 4.667271627344223e-05,
      "loss": 3.8627,
      "step": 110
    },
    {
      "epoch": 0.2177858439201452,
      "grad_norm": 1.35114324092865,
      "learning_rate": 4.6370235934664246e-05,
      "loss": 3.8825,
      "step": 120
    },
    {
      "epoch": 0.23593466424682397,
      "grad_norm": 2.0628280639648438,
      "learning_rate": 4.606775559588627e-05,
      "loss": 3.8339,
      "step": 130
    },
    {
      "epoch": 0.2540834845735027,
      "grad_norm": 0.7266502976417542,
      "learning_rate": 4.576527525710829e-05,
      "loss": 3.9223,
      "step": 140
    },
    {
      "epoch": 0.27223230490018147,
      "grad_norm": 1.9504543542861938,
      "learning_rate": 4.546279491833031e-05,
      "loss": 3.8573,
      "step": 150
    },
    {
      "epoch": 0.29038112522686027,
      "grad_norm": 0.5992074012756348,
      "learning_rate": 4.516031457955233e-05,
      "loss": 3.8262,
      "step": 160
    },
    {
      "epoch": 0.308529945553539,
      "grad_norm": 1.187515139579773,
      "learning_rate": 4.4857834240774356e-05,
      "loss": 3.8356,
      "step": 170
    },
    {
      "epoch": 0.32667876588021777,
      "grad_norm": 0.5904196500778198,
      "learning_rate": 4.4555353901996375e-05,
      "loss": 3.7731,
      "step": 180
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 0.7084171175956726,
      "learning_rate": 4.4252873563218394e-05,
      "loss": 3.7896,
      "step": 190
    },
    {
      "epoch": 0.3629764065335753,
      "grad_norm": 0.7318955063819885,
      "learning_rate": 4.395039322444041e-05,
      "loss": 3.7766,
      "step": 200
    },
    {
      "epoch": 0.3811252268602541,
      "grad_norm": 0.9328500628471375,
      "learning_rate": 4.364791288566243e-05,
      "loss": 3.8456,
      "step": 210
    },
    {
      "epoch": 0.3992740471869328,
      "grad_norm": 0.7043491005897522,
      "learning_rate": 4.334543254688446e-05,
      "loss": 3.8106,
      "step": 220
    },
    {
      "epoch": 0.41742286751361163,
      "grad_norm": 0.6526510119438171,
      "learning_rate": 4.304295220810647e-05,
      "loss": 3.7714,
      "step": 230
    },
    {
      "epoch": 0.4355716878402904,
      "grad_norm": 2.562448263168335,
      "learning_rate": 4.27404718693285e-05,
      "loss": 3.7708,
      "step": 240
    },
    {
      "epoch": 0.4537205081669691,
      "grad_norm": 1.6555784940719604,
      "learning_rate": 4.2437991530550516e-05,
      "loss": 3.7482,
      "step": 250
    },
    {
      "epoch": 0.47186932849364793,
      "grad_norm": 1.12593674659729,
      "learning_rate": 4.2135511191772535e-05,
      "loss": 3.8102,
      "step": 260
    },
    {
      "epoch": 0.4900181488203267,
      "grad_norm": 0.697059154510498,
      "learning_rate": 4.183303085299456e-05,
      "loss": 3.7176,
      "step": 270
    },
    {
      "epoch": 0.5081669691470054,
      "grad_norm": 0.7146427631378174,
      "learning_rate": 4.1530550514216574e-05,
      "loss": 3.7237,
      "step": 280
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 2.280665874481201,
      "learning_rate": 4.12280701754386e-05,
      "loss": 3.7204,
      "step": 290
    },
    {
      "epoch": 0.5444646098003629,
      "grad_norm": 0.753576934337616,
      "learning_rate": 4.092558983666062e-05,
      "loss": 3.7668,
      "step": 300
    },
    {
      "epoch": 0.5626134301270418,
      "grad_norm": 0.7644019722938538,
      "learning_rate": 4.062310949788264e-05,
      "loss": 3.7578,
      "step": 310
    },
    {
      "epoch": 0.5807622504537205,
      "grad_norm": 1.3847161531448364,
      "learning_rate": 4.0320629159104664e-05,
      "loss": 3.6961,
      "step": 320
    },
    {
      "epoch": 0.5989110707803993,
      "grad_norm": 0.7148539423942566,
      "learning_rate": 4.0018148820326676e-05,
      "loss": 3.7279,
      "step": 330
    },
    {
      "epoch": 0.617059891107078,
      "grad_norm": 1.299643874168396,
      "learning_rate": 3.97156684815487e-05,
      "loss": 3.6288,
      "step": 340
    },
    {
      "epoch": 0.6352087114337568,
      "grad_norm": 1.1915212869644165,
      "learning_rate": 3.941318814277072e-05,
      "loss": 3.5826,
      "step": 350
    },
    {
      "epoch": 0.6533575317604355,
      "grad_norm": 0.6496710181236267,
      "learning_rate": 3.911070780399274e-05,
      "loss": 3.7346,
      "step": 360
    },
    {
      "epoch": 0.6715063520871143,
      "grad_norm": 0.5179330706596375,
      "learning_rate": 3.8808227465214767e-05,
      "loss": 3.7003,
      "step": 370
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 2.5161304473876953,
      "learning_rate": 3.850574712643678e-05,
      "loss": 3.7124,
      "step": 380
    },
    {
      "epoch": 0.7078039927404719,
      "grad_norm": 2.926274299621582,
      "learning_rate": 3.8203266787658805e-05,
      "loss": 3.6245,
      "step": 390
    },
    {
      "epoch": 0.7259528130671506,
      "grad_norm": 0.6281072497367859,
      "learning_rate": 3.7900786448880824e-05,
      "loss": 3.6278,
      "step": 400
    },
    {
      "epoch": 0.7441016333938294,
      "grad_norm": 0.8022981286048889,
      "learning_rate": 3.759830611010284e-05,
      "loss": 3.7318,
      "step": 410
    },
    {
      "epoch": 0.7622504537205081,
      "grad_norm": 0.7545880079269409,
      "learning_rate": 3.729582577132487e-05,
      "loss": 3.6666,
      "step": 420
    },
    {
      "epoch": 0.7803992740471869,
      "grad_norm": 0.853765606880188,
      "learning_rate": 3.699334543254689e-05,
      "loss": 3.7211,
      "step": 430
    },
    {
      "epoch": 0.7985480943738656,
      "grad_norm": 0.7259724736213684,
      "learning_rate": 3.669086509376891e-05,
      "loss": 3.7415,
      "step": 440
    },
    {
      "epoch": 0.8166969147005445,
      "grad_norm": 0.5746471881866455,
      "learning_rate": 3.638838475499093e-05,
      "loss": 3.665,
      "step": 450
    },
    {
      "epoch": 0.8348457350272233,
      "grad_norm": 0.7912384867668152,
      "learning_rate": 3.6085904416212946e-05,
      "loss": 3.6842,
      "step": 460
    },
    {
      "epoch": 0.852994555353902,
      "grad_norm": 0.8795621991157532,
      "learning_rate": 3.5783424077434965e-05,
      "loss": 3.6106,
      "step": 470
    },
    {
      "epoch": 0.8711433756805808,
      "grad_norm": 0.7526568174362183,
      "learning_rate": 3.548094373865699e-05,
      "loss": 3.6482,
      "step": 480
    },
    {
      "epoch": 0.8892921960072595,
      "grad_norm": 1.8730974197387695,
      "learning_rate": 3.517846339987901e-05,
      "loss": 3.6359,
      "step": 490
    },
    {
      "epoch": 0.9074410163339383,
      "grad_norm": 0.47994139790534973,
      "learning_rate": 3.487598306110103e-05,
      "loss": 3.6533,
      "step": 500
    },
    {
      "epoch": 0.925589836660617,
      "grad_norm": 0.4946575462818146,
      "learning_rate": 3.4573502722323056e-05,
      "loss": 3.6226,
      "step": 510
    },
    {
      "epoch": 0.9437386569872959,
      "grad_norm": 0.80926513671875,
      "learning_rate": 3.427102238354507e-05,
      "loss": 3.5995,
      "step": 520
    },
    {
      "epoch": 0.9618874773139746,
      "grad_norm": 0.5568594336509705,
      "learning_rate": 3.3968542044767094e-05,
      "loss": 3.6845,
      "step": 530
    },
    {
      "epoch": 0.9800362976406534,
      "grad_norm": 0.5997382402420044,
      "learning_rate": 3.366606170598911e-05,
      "loss": 3.6097,
      "step": 540
    },
    {
      "epoch": 0.9981851179673321,
      "grad_norm": 0.6166577339172363,
      "learning_rate": 3.336358136721113e-05,
      "loss": 3.644,
      "step": 550
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.4125773906707764,
      "eval_runtime": 46.4051,
      "eval_samples_per_second": 23.747,
      "eval_steps_per_second": 2.974,
      "step": 551
    },
    {
      "epoch": 1.0163339382940109,
      "grad_norm": 2.49190092086792,
      "learning_rate": 3.306110102843316e-05,
      "loss": 3.5806,
      "step": 560
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 0.6994803547859192,
      "learning_rate": 3.275862068965517e-05,
      "loss": 3.6109,
      "step": 570
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 1.1906269788742065,
      "learning_rate": 3.24561403508772e-05,
      "loss": 3.5942,
      "step": 580
    },
    {
      "epoch": 1.0707803992740472,
      "grad_norm": 0.6217576861381531,
      "learning_rate": 3.2153660012099216e-05,
      "loss": 3.6084,
      "step": 590
    },
    {
      "epoch": 1.0889292196007259,
      "grad_norm": 0.6233581304550171,
      "learning_rate": 3.1851179673321235e-05,
      "loss": 3.5985,
      "step": 600
    },
    {
      "epoch": 1.1070780399274047,
      "grad_norm": 0.5822057127952576,
      "learning_rate": 3.154869933454326e-05,
      "loss": 3.5903,
      "step": 610
    },
    {
      "epoch": 1.1252268602540836,
      "grad_norm": 0.664372980594635,
      "learning_rate": 3.1246218995765273e-05,
      "loss": 3.6317,
      "step": 620
    },
    {
      "epoch": 1.1433756805807622,
      "grad_norm": 0.6147598028182983,
      "learning_rate": 3.09437386569873e-05,
      "loss": 3.5174,
      "step": 630
    },
    {
      "epoch": 1.161524500907441,
      "grad_norm": 1.080549716949463,
      "learning_rate": 3.064125831820932e-05,
      "loss": 3.6408,
      "step": 640
    },
    {
      "epoch": 1.1796733212341197,
      "grad_norm": 0.5605412721633911,
      "learning_rate": 3.0338777979431338e-05,
      "loss": 3.6204,
      "step": 650
    },
    {
      "epoch": 1.1978221415607986,
      "grad_norm": 0.6977529525756836,
      "learning_rate": 3.003629764065336e-05,
      "loss": 3.6412,
      "step": 660
    },
    {
      "epoch": 1.2159709618874772,
      "grad_norm": 0.8396897315979004,
      "learning_rate": 2.9733817301875376e-05,
      "loss": 3.5992,
      "step": 670
    },
    {
      "epoch": 1.234119782214156,
      "grad_norm": 0.5859823226928711,
      "learning_rate": 2.94313369630974e-05,
      "loss": 3.6172,
      "step": 680
    },
    {
      "epoch": 1.252268602540835,
      "grad_norm": 0.6322213411331177,
      "learning_rate": 2.912885662431942e-05,
      "loss": 3.6515,
      "step": 690
    },
    {
      "epoch": 1.2704174228675136,
      "grad_norm": 0.753584623336792,
      "learning_rate": 2.882637628554144e-05,
      "loss": 3.5433,
      "step": 700
    },
    {
      "epoch": 1.2885662431941924,
      "grad_norm": 0.8559590578079224,
      "learning_rate": 2.8523895946763463e-05,
      "loss": 3.5547,
      "step": 710
    },
    {
      "epoch": 1.306715063520871,
      "grad_norm": 2.1317429542541504,
      "learning_rate": 2.822141560798548e-05,
      "loss": 3.6195,
      "step": 720
    },
    {
      "epoch": 1.32486388384755,
      "grad_norm": 0.7304884791374207,
      "learning_rate": 2.79189352692075e-05,
      "loss": 3.6583,
      "step": 730
    },
    {
      "epoch": 1.3430127041742286,
      "grad_norm": 0.5817313194274902,
      "learning_rate": 2.7616454930429524e-05,
      "loss": 3.6023,
      "step": 740
    },
    {
      "epoch": 1.3611615245009074,
      "grad_norm": 0.5442840456962585,
      "learning_rate": 2.7313974591651543e-05,
      "loss": 3.6155,
      "step": 750
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 1.1652781963348389,
      "learning_rate": 2.7011494252873566e-05,
      "loss": 3.5915,
      "step": 760
    },
    {
      "epoch": 1.397459165154265,
      "grad_norm": 0.9203743934631348,
      "learning_rate": 2.670901391409559e-05,
      "loss": 3.5933,
      "step": 770
    },
    {
      "epoch": 1.4156079854809438,
      "grad_norm": 0.6756303310394287,
      "learning_rate": 2.6406533575317604e-05,
      "loss": 3.5883,
      "step": 780
    },
    {
      "epoch": 1.4337568058076224,
      "grad_norm": 0.9655542969703674,
      "learning_rate": 2.6104053236539627e-05,
      "loss": 3.5618,
      "step": 790
    },
    {
      "epoch": 1.4519056261343013,
      "grad_norm": 0.732298731803894,
      "learning_rate": 2.5801572897761646e-05,
      "loss": 3.6012,
      "step": 800
    },
    {
      "epoch": 1.47005444646098,
      "grad_norm": 1.1849783658981323,
      "learning_rate": 2.549909255898367e-05,
      "loss": 3.5937,
      "step": 810
    },
    {
      "epoch": 1.4882032667876588,
      "grad_norm": 0.6391147375106812,
      "learning_rate": 2.519661222020569e-05,
      "loss": 3.6273,
      "step": 820
    },
    {
      "epoch": 1.5063520871143377,
      "grad_norm": 0.6433192491531372,
      "learning_rate": 2.489413188142771e-05,
      "loss": 3.5625,
      "step": 830
    },
    {
      "epoch": 1.5245009074410163,
      "grad_norm": 0.6492921710014343,
      "learning_rate": 2.459165154264973e-05,
      "loss": 3.5061,
      "step": 840
    },
    {
      "epoch": 1.542649727767695,
      "grad_norm": 1.044684648513794,
      "learning_rate": 2.428917120387175e-05,
      "loss": 3.5946,
      "step": 850
    },
    {
      "epoch": 1.560798548094374,
      "grad_norm": 0.7484861016273499,
      "learning_rate": 2.3986690865093768e-05,
      "loss": 3.6043,
      "step": 860
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 0.5364334583282471,
      "learning_rate": 2.368421052631579e-05,
      "loss": 3.5997,
      "step": 870
    },
    {
      "epoch": 1.5970961887477313,
      "grad_norm": 1.4664167165756226,
      "learning_rate": 2.3381730187537813e-05,
      "loss": 3.6671,
      "step": 880
    },
    {
      "epoch": 1.6152450090744102,
      "grad_norm": 0.61865234375,
      "learning_rate": 2.3079249848759832e-05,
      "loss": 3.5303,
      "step": 890
    },
    {
      "epoch": 1.633393829401089,
      "grad_norm": 0.6704695820808411,
      "learning_rate": 2.277676950998185e-05,
      "loss": 3.5486,
      "step": 900
    },
    {
      "epoch": 1.6515426497277677,
      "grad_norm": 0.6193209886550903,
      "learning_rate": 2.247428917120387e-05,
      "loss": 3.5624,
      "step": 910
    },
    {
      "epoch": 1.6696914700544465,
      "grad_norm": 0.5357956290245056,
      "learning_rate": 2.2171808832425893e-05,
      "loss": 3.5748,
      "step": 920
    },
    {
      "epoch": 1.6878402903811254,
      "grad_norm": 1.273820400238037,
      "learning_rate": 2.1869328493647916e-05,
      "loss": 3.5273,
      "step": 930
    },
    {
      "epoch": 1.705989110707804,
      "grad_norm": 1.0342329740524292,
      "learning_rate": 2.1566848154869935e-05,
      "loss": 3.5646,
      "step": 940
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 0.5863909721374512,
      "learning_rate": 2.1264367816091954e-05,
      "loss": 3.5979,
      "step": 950
    },
    {
      "epoch": 1.7422867513611615,
      "grad_norm": 1.0763499736785889,
      "learning_rate": 2.0961887477313977e-05,
      "loss": 3.5569,
      "step": 960
    },
    {
      "epoch": 1.7604355716878404,
      "grad_norm": 1.1136671304702759,
      "learning_rate": 2.0659407138535996e-05,
      "loss": 3.5633,
      "step": 970
    },
    {
      "epoch": 1.778584392014519,
      "grad_norm": 3.3698275089263916,
      "learning_rate": 2.0356926799758015e-05,
      "loss": 3.5241,
      "step": 980
    },
    {
      "epoch": 1.7967332123411979,
      "grad_norm": 1.552578330039978,
      "learning_rate": 2.0054446460980038e-05,
      "loss": 3.5994,
      "step": 990
    },
    {
      "epoch": 1.8148820326678767,
      "grad_norm": 0.6396034955978394,
      "learning_rate": 1.975196612220206e-05,
      "loss": 3.5523,
      "step": 1000
    },
    {
      "epoch": 1.8330308529945554,
      "grad_norm": 0.5268221497535706,
      "learning_rate": 1.944948578342408e-05,
      "loss": 3.5242,
      "step": 1010
    },
    {
      "epoch": 1.851179673321234,
      "grad_norm": 0.9812537431716919,
      "learning_rate": 1.91470054446461e-05,
      "loss": 3.5501,
      "step": 1020
    },
    {
      "epoch": 1.8693284936479129,
      "grad_norm": 1.0986837148666382,
      "learning_rate": 1.8844525105868118e-05,
      "loss": 3.592,
      "step": 1030
    },
    {
      "epoch": 1.8874773139745917,
      "grad_norm": 1.0920253992080688,
      "learning_rate": 1.854204476709014e-05,
      "loss": 3.5898,
      "step": 1040
    },
    {
      "epoch": 1.9056261343012704,
      "grad_norm": 0.7427436113357544,
      "learning_rate": 1.8239564428312163e-05,
      "loss": 3.5216,
      "step": 1050
    },
    {
      "epoch": 1.9237749546279492,
      "grad_norm": 0.6653650999069214,
      "learning_rate": 1.7937084089534182e-05,
      "loss": 3.5732,
      "step": 1060
    },
    {
      "epoch": 1.941923774954628,
      "grad_norm": 0.5983993411064148,
      "learning_rate": 1.76346037507562e-05,
      "loss": 3.5238,
      "step": 1070
    },
    {
      "epoch": 1.9600725952813067,
      "grad_norm": 0.7065795660018921,
      "learning_rate": 1.733212341197822e-05,
      "loss": 3.4521,
      "step": 1080
    },
    {
      "epoch": 1.9782214156079854,
      "grad_norm": 0.648957371711731,
      "learning_rate": 1.7029643073200243e-05,
      "loss": 3.5971,
      "step": 1090
    },
    {
      "epoch": 1.9963702359346642,
      "grad_norm": 0.7250194549560547,
      "learning_rate": 1.6727162734422262e-05,
      "loss": 3.5346,
      "step": 1100
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.333712100982666,
      "eval_runtime": 49.269,
      "eval_samples_per_second": 22.367,
      "eval_steps_per_second": 2.801,
      "step": 1102
    }
  ],
  "logging_steps": 10,
  "max_steps": 1653,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 596315977285632.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
